# -*- coding: utf-8 -*-
"""Lab_Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bc_FvrA01IHexMAw2w6HIeoRcmdcKh-p
"""

import os
import sklearn
import pandas as pd
import numpy as np, pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from google.colab import drive
import seaborn as sns
import tensorflow as tf
from keras.models import save_model
from tensorflow import keras
from sklearn.metrics import roc_auc_score
from sklearn.ensemble import RandomForestClassifier
import pickle
from keras.layers import Input, Dense, concatenate
from sklearn.metrics import accuracy_score
from keras.models import Model
from sklearn.model_selection import GridSearchCV
from imblearn.over_sampling import RandomOverSampler
from tensorflow.keras.layers import Dropout
drive.mount('/content/drive')

"""**DATA PREPROCESSING**"""

df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/intro_to_AI/CustomerChurn_dataset.csv')

df

#checking for missing values among the columns with numeric values
numeric_columns = df.select_dtypes(include=['float64','int64'])
missing_values = numeric_columns.isnull().sum()
missing_values

#checking for missing values among the columns with categorical values
objects_columns = df.select_dtypes(include=['object'])
objects_columns

objects_columns.info()

##SINCE THERE ARE NO NULL VALUES TO HANDLE, WE PROCEED TO ENCODING OF CATEGORICAL VALUES & IMPUTING

# Convert the 'TotalCharges' column to a numeric data type and handle invalid values by replacing them with 0
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')

df['TotalCharges'] = df['TotalCharges'].fillna(0)

df.info()

# To determine which encoding methods was suitable for which columns
for column_name in df.columns:
    unique_values = df[column_name].unique()
    print(f'Unique values in column "{column_name}": {unique_values}')

# Label encoding is used for the following columns due to its ordinal nature, reflecting the clear order of categories.
categorical_cols_unique = ['gender', 'Partner', 'Dependents', 'PhoneService',  'Contract', 'PaperlessBilling', 'Churn']
label_encoder = LabelEncoder()

# Iterate through the columns and apply label encoding
for column in categorical_cols_unique:
    df[column] = label_encoder.fit_transform(df[column])

pickle_out = open("label_encoder.pkl", "wb")
pickle.dump(label_encoder, pickle_out)
pickle_out.close()

# One-hot encoding is employed for the following columns to prevent any implied order, as it is nominal data.
categorical_cols = ['MultipleLines', 'InternetService', 'OnlineSecurity',
                    'OnlineBackup', 'DeviceProtection', 'TechSupport',
                    'StreamingTV','StreamingMovies', 'PaymentMethod']

df = pd.get_dummies(df, columns=categorical_cols)
df

df.info()

##SCALING OF NUMERICAL VALUES

scaler = StandardScaler()
columns_to_scale = ['tenure', 'MonthlyCharges', 'SeniorCitizen', 'TotalCharges']

df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])

pickle_out = open("scaler.pkl", "wb")
pickle.dump(scaler, pickle_out)
pickle_out.close()

df

"""**FEATURE IMPORTANCE & EDA**"""

df.info()

#Since customerID has no correlation with our target variable churn, we proceed to drop it from thr table
df = df.drop(['customerID'], axis=1)
df

# We use fature immportance to figure out the right features to train the model

X= df.drop(['Churn'], axis= 1).values
y = pd.get_dummies(df['Churn'], prefix=['Churn', 'Not Churn'])

rf_clf = RandomForestClassifier()
rf_clf.fit(X, y)
feature_importance_rf = rf_clf.feature_importances_

plt.figure(figsize=(10, 6))
sns.barplot(x=feature_importance_rf, y=df.drop(['Churn'], axis=1).columns)
plt.title('Random Forest Feature Importance')
plt.show()

# Set the threshold for feature importance
threshold = 0.02

# Get the indices of features with importance scores above the threshold
important_feature_indices = np.where(feature_importance_rf > threshold)[0]

# Get the corresponding column names
important_feature_names = df.drop(['Churn'], axis=1).columns[important_feature_indices]

# Display the important features and their importance scores
for name, importance in zip(important_feature_names, feature_importance_rf[important_feature_indices]):
    print(f"{name}: {importance}")

##EDA for some of the impotant features (selected independent variables) to find out which customer profiles relate to churning a lot

# Define a function for creating a boxplot
def plot_box(x, y, data, xlabel, ylabel, title):
    sns.boxplot(x=x, y=y, data=data, palette='husl')
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.title(title)
    plt.show()

# Box plot for Monthly Charges by Churn
plot_box("Churn", "MonthlyCharges", df, "Churn", "Monthly Charges", "Monthly Charges by Churn")

# Box plot for Total Charges by Churn
plot_box("Churn", "TotalCharges", df, "Churn", "Total Charges", "Total Charges by Churn")

# Box plot for Tenure by Churn
plot_box("Churn", "tenure", df, "Churn", "Tenure", "Tenure by Churn")

# Define a function for creating a count plot
def plot_count(x, hue, data, xlabel, title):
    sns.countplot(x=x, hue=hue, data=data, palette='husl')
    plt.xlabel(xlabel)
    plt.title(title)
    plt.show()

# Count plot for Contract type by Churn
plot_count("Contract", "Churn", df, "Contract Type", "Churn Count by Contract Type")

# Count plot for PaperlessBilling by Churn
plot_count("PaperlessBilling", "Churn", df, "Paperless Billing", "Churn Count by Paperless Billing")

# Count plot for Internet Service type by Churn
plot_count("InternetService_Fiber optic", "Churn", df, "Internet Service Type", "Churn Count by Internet Service Type")

# Count plot for Online Security by Churn
plot_count("OnlineSecurity_No", "Churn", df, "Online Security", "Churn Count by Online Security")

# Count plot for Tech Support by Churn
plot_count("TechSupport_No", "Churn", df, "Tech Support", "Churn Count by Tech Support")

# Count plot for Payment Method by Churn
plot_count("PaymentMethod_Electronic check", "Churn", df, "Payment Method", "Churn Count by Payment Method")

# Count plot for Senior Citizen by Churn
plot_count("SeniorCitizen", "Churn", df, "Senior Citizen", "Churn Count by Senior Citizen")

"""**MODEL TRAINING**"""

#After feature importance, we realized our columns needed and placed all of them in an array
selected_columns = [
    'gender', 'SeniorCitizen', 'Partner', 'tenure', 'Contract', 'PaperlessBilling',
    'MonthlyCharges', 'TotalCharges', 'InternetService_Fiber optic',
    'OnlineSecurity_No', 'TechSupport_No', 'PaymentMethod_Electronic check'
]


X= df[selected_columns].values
y= df['Churn'].values

df.info()

"""**TRAINING, TESTING & CROSSVALIDATION WITH GRIDSEARCHCV**"""

# Split your data into training and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)
X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Define input layer
inputs = keras.Input(shape=(12,))

# Build the model using Functional API with dropout
x = keras.layers.Dense(units=20, activation='relu')(inputs)
x = keras.layers.Dense(units=20, activation='relu')(x)
x = keras.layers.Dense(units=20, activation='relu')(x)
outputs = keras.layers.Dense(units=1, activation='sigmoid')(x)

model = keras.Model(inputs=inputs, outputs=outputs)

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model using the resampled training data
history = model.fit(X_train, y_train, batch_size=32, epochs=25, validation_data=(X_val, y_val))

# Plot the training and validation accuracies
plt.figure(figsize=(10, 6))
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracies')
plt.legend()
plt.show()

# Evaluate the model on the test set
test_accuracy = model.evaluate(X_test, y_test, verbose=0)[1]
print(f"Test accuracy: {test_accuracy}")

print(f"Training accuracy: {history.history['accuracy'][-1]}")
print(f"Validation accuracy: {history.history['val_accuracy'][-1]}")

#Testing the model
predictions = model.predict(X_test)

#Double-check if the true values and the predicted values have any null values
nan_in_y_train = pd.isna(y_train)
nan_in_predictions = pd.isna(predictions)

print("NaN values in y_train:", nan_in_y_train.any())
print("NaN values in predictions:", nan_in_predictions.any())

#Evaluated the model’s accuracy, calculate the AUC score
auc_score = roc_auc_score(y_test, predictions)
auc_score

"""**OPTIMIZATION (HYPER PARAMETER TUNING)**"""

param_grid = {
    'batch_size': [24, 32],
    'epochs': [25, 50],
    'units': [10, 20, 30],
    'activation': ['selu','relu'],
    'optimizer': ['adam'],
}


best_accuracy = 0
best_hyperparameters = None

training_accuracies = []
validation_accuracies = []

for units in param_grid['units']:
    for activation in param_grid['activation']:
        for optimizer in param_grid['optimizer']:
            for batch_size in param_grid['batch_size']:
                for epochs in param_grid['epochs']:
                    # Define input layer
                    inputs = Input(shape=(12,))

                    # Build the model using Functional API
                    x = Dense(units=units, activation=activation)(inputs)
                    x = Dense(units=units, activation=activation)(x)
                    x = Dense(units=units, activation=activation)(x)
                    outputs = Dense(units=1, activation='sigmoid')(x)

                    model = keras.Model(inputs=inputs, outputs=outputs)

                    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

                    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val))

                    # Append accuracies to the lists
                    training_accuracies.append(history.history['accuracy'][-1])
                    validation_accuracies.append(history.history['val_accuracy'][-1])

                    if history.history['val_accuracy'][-1] > best_accuracy:
                        best_hyperparameters = (units, activation, optimizer, batch_size, epochs)
                        best_accuracy = history.history['val_accuracy'][-1]
                        best_model1 = model

# Plot the best training and validation accuracies
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(training_accuracies) + 1), training_accuracies, label='Training Accuracy')
plt.plot(range(1, len(validation_accuracies) + 1), validation_accuracies, label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracies during Grid Search & Hyper-parameter Tuning')
plt.legend()
plt.show()


# Evaluate the best model on the test set
test_accuracy = best_model1.evaluate(X_test, y_test, verbose=0)[1]
print(f"Test accuracy using the best model: {test_accuracy}")

print(f"Best training accuracy: {history.history['accuracy'][-1]}")
print(f"Best validation accuracy: {history.history['val_accuracy'][-1]}")
print(f"Best hyperparameters: {best_hyperparameters}")

#Test the new model
predictions = best_model1.predict(X_test)

#Evaluated the new retrained model’s accuracy, calculate the AUC score
auc_score2 = roc_auc_score(y_test, predictions)
auc_score2

best_model1.save("best_model1.h5")